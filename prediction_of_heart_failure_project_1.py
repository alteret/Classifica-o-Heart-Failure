# -*- coding: utf-8 -*-
"""prediction-of-heart-failure-project-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPLV-PC1vTu-fNxj8jq3wr8snW5CNHjv
"""

#Importing the useful libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score
from imblearn.under_sampling import RandomUnderSampler
import scikitplot as skplt

"""## In this notebook I want to predict if some person with some caracteristics has chance to death or not. But before that, let's explore the dataset.

## Note: In this project I won't explore the relation between the variables and the target because my interest is prediction and not inference. But, I will leave some links that could help to understand each variable and its relation with the heart failure:

#### creatinine_phosphokinase [Creatinine Phosphokinase]: https://www.mountsinai.org/health-library/tests/creatine-phosphokinase-test
#### diabetes [Diabetes]: https://www.heart.org/en/news/2019/06/06/diabetes-and-heart-failure-are-linked-treatment-should-be-too#:~:text=People%20who%20have%20Type%202,a%20risk%20factor%20for%20diabetes.
#### ejection_fraction [Ejection Fraction]: https://www.heart.org/en/health-topics/heart-failure/diagnosing-heart-failure/ejection-fraction-heart-failure-measurement#:~:text=A%20ejection%20fraction%20measurement%20under,from%20a%20previous%20heart%20attack.
#### high_blood_pressure [High Blood Pressure]: https://www.webmd.com/heart-disease/heart-failure/blood-pressure-heart-failure#:~:text=If%20you%20have%20heart%20failure,risk%20factor%20for%20heart%20failure.
#### platelets [Platelets]: https://www.ahajournals.org/doi/full/10.1161/01.CIR.0000086897.15588.4B
#### serum_creatinine [Serum Creatinine]: https://www.onlinejcf.com/article/S1071-9164(02)25410-X/fulltext#:~:text=We%20believe%20the%20more%20likely,tolerate%20inpatient%20heart%20failure%20treatment.
#### serum_sodium [Serum Sodium]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6224129/#:~:text=Hyponatremia%20or%20low%20serum%20sodium,or%20reducing%20the%20stroke%20volume.

# Work Flow


1. Exploratory Data Analysis
    - Correlation
    - Distribution
2. Fitting the Data With Logistic Regression.
    - Unbalanced Data
    - Balanced Data
3. Fitting the Data With Model of Decision Tree
    - Balanced Data
    - Unbalanced Data
4. Conclusion

# Exploratory Data Analysis
"""

#Loading the data
df = pd.read_csv("/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv")

# Printing the first five rows
df.head()

#Renaming our main variable
df.rename(columns={'DEATH_EVENT':'heart_failure'}, inplace=True)

df.columns

#I use "diag_kind = None" for appear the first graph.
sns.pairplot(df, x_vars = ['age', 'creatinine_phosphokinase',
       'ejection_fraction', 'platelets',
       'serum_creatinine', 'serum_sodium', 'time'], y_vars= ['heart_failure'],kind='reg', diag_kind = None)

#Verifying if there are nulls.
df.isnull().sum()

# Importando o Pandas Profiling
from pandas_profiling import ProfileReport

#Loading the Pandas Profiling Report 

profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})
profile.to_notebook_iframe()

"""## Correlation

### Correlation corresponds how much a variable is conected with other one. Suppose a linear function as follows: y = βx, where y and x are variables and β is the slope coefficient. The higher the β, the x is more correlated with y.

### High correlation can underperform our model, so we have to take care about it.

### In this project I will consider a high correlation which is more than 70%. By the *pandas profile report* we have saw the variables don't have any correlations more than 70%. The most is 45% with *smoke* and *sex*, look: 
"""

#Loading the correlation table

df_corr = df.corr()
df_corr

# Loading the correlation table to level higher than 0.3

df_corr[df_corr > 0.3]

"""## Distribution
### The variable distribution is important for us because it represents the randomness of our data. We need of data distributed to have goods independents variables to predict our dependent variable.

### There are two reason to remove variables with low distribution:
1. Variable is random, but it won't help us to determinate our dependent variable because the values of this variable are not so distributed as it is necessary to generate an intelligence from them.
2. Variable is not random, so it's not significant to determinate our dependent variable

### We have saw too that the variables have a good distribution at the *pandas profile report*. I put a limit of 70-30 for a good distribution, because a concentration more than 70% can give us some problem.

## Let's look specially to our target.
"""

df['heart_failure'].mean()

"""### We can have a problem with the distribution of the target "heart_failure". In our dataset almost 70% have a value 0, that is, haven't had heart failure. 

### How we want to classificate between two class ("have had heart failure" and "haven't had heart failure"), maybe our model gives 70% of accuracy, but it gives us only value 0. It's not a good model, it always will return only one class. 

### We call it of *false alarm*. We can fix it when run our model. In advance, I'll use the method *under_sampling*. But, I want to show the results with unbalanced and balanced data for comparison!
"""

# Spliting the data 

X = df.drop(["heart_failure"], axis=1)
y = df["heart_failure"]

x_train, x_test, y_train, y_test =  train_test_split(X, y, random_state=1)

"""# Fitting the data with Logistic Regression

## Unbalanced Data
"""

lreg = LogisticRegression(random_state=1)

lreg.fit(x_train, y_train)

y_pred =  lreg.predict(x_test)
print("Relatório de Classificação:\n", classification_report(y_test, y_pred, digits=4))

#plotar a matrix de confusão
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)

lreg.coef_

"""### We observed that we got a good accuracy. It's good, but it could be a false alarm.

### Let's make the balance and see the results. How I have said, let's use the *under_sampling*.

## Data Balanced
### Under_sampling remove some observations of the class that appear more and balance with the class with less frequency.
"""

from imblearn.under_sampling import RandomUnderSampler

# usar técnica under-sampling
rus = RandomUnderSampler()
X_res, y_res = rus.fit_sample(x_train, y_train)
 
# ver o balanceamento das classes
print(pd.Series(y_res).value_counts())
 
# plotar a nova distribuição de classes
sns.countplot(y_res);

lreg_us = LogisticRegression(random_state=1)
lreg_us.fit(X_res, y_res)

y_pred = lreg_us.predict(x_test)

print("Relatório de Classificação:\n", classification_report(y_test, y_pred, digits=4))
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)

"""# Fitting the data with Model of Decision Tree

## Unbalanced Data
"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(random_state=1)
dtree.fit(x_train, y_train)

y_pred = dtree.predict(x_test)
print("Relatório de Classificação:\n", classification_report(y_test, y_pred, digits=4))
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)

"""## Balanced Data"""

dtree_us = DecisionTreeClassifier(random_state=1)
dtree_us.fit(X_res, y_res)

y_pred = dtree_us.predict(x_test)
print("Relatório de Classificação:\n", classification_report(y_test, y_pred, digits=4))
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)

"""# Conclusion

### In our case, what matter? We need more precision in the class 1, the class that indicate a high probability of heart failure.

### Imagine that a hospital have a treatment more advanced for people who signal to has a big chance to has heart failure. If the hospital decide to use the model, it's better suggest the treatment for someone who actually hasn't chance make the treatment than not to suggest for someone who has a high chance. In this context, the model balanced is better. So, we will use the model with more precision.

### So, the better model is logistic regression with unbalanced data.
"""

